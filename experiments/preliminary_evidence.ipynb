{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc820c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    l: nn.ModuleList\n",
    "    f: nn.Module\n",
    "\n",
    "    def __init__(self, *sizes: int, f: nn.Module = nn.ReLU()):\n",
    "        super().__init__() # type: ignore\n",
    "        if len(sizes) < 2:\n",
    "            raise ValueError(f\"`sizes` must contain at least 2 elements (input size and at least one output size), got {sizes} instead.\")\n",
    "        self.f = f\n",
    "        self.l = nn.ModuleList()\n",
    "        for i in range(len(sizes) - 1):\n",
    "            self.l.append(nn.Linear(sizes[i], sizes[i+1]))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for i, l in enumerate(self.l):\n",
    "            x = l(x)\n",
    "            if i < len(self.l) - 1:\n",
    "                x = self.f(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb40420c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(\n",
    "        X: np.typing.ArrayLike, y: np.typing.ArrayLike, \n",
    "        model: nn.Module, loss_fn: nn.Module, \n",
    "        optim: torch.optim.Optimizer, epochs: int = 50, step: float = .2,\n",
    "        verbose: bool = True\n",
    "    ) -> None:\n",
    "    X = torch.tensor(X, dtype=torch.float32) # type: ignore\n",
    "    y = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    for _ in (bar := tqdm(range(epochs), desc=\"Training\", unit=\"epoch\", disable=not verbose)):\n",
    "        y_ = model(X)\n",
    "        loss = loss_fn(y, y_)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        bar.set_description(f\"Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8ece3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN:\n",
    "    def __init__(self, hidden_layer_sizes=(100,), activation=nn.ReLU(), \n",
    "                 loss_fn=nn.MSELoss(), optimizer=torch.optim.Adam, lr=0.001, \n",
    "                 epochs=50, step=0.2, verbose=True):\n",
    "        \"\"\"\n",
    "        A scikit-learn compatible feed-forward neural network using PyTorch.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        hidden_layer_sizes : tuple\n",
    "            The size of hidden layers.\n",
    "        activation : nn.Module\n",
    "            The activation function for the hidden layers.\n",
    "        loss_fn : nn.Module\n",
    "            The loss function to use.\n",
    "        optimizer : torch.optim.Optimizer\n",
    "            The optimizer class to use.\n",
    "        lr : float\n",
    "            The learning rate for the optimizer.\n",
    "        epochs : int\n",
    "            Number of training epochs.\n",
    "        step : float\n",
    "            Step size parameter passed to train function.\n",
    "        verbose : bool\n",
    "            Whether to display training progress.\n",
    "        \"\"\"\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.activation = activation\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.step = step\n",
    "        self.verbose = verbose\n",
    "        self.model = None\n",
    "        self.is_fitted_ = False\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the model to the data.\"\"\"\n",
    "        # Convert y to 2D if needed\n",
    "        y = np.atleast_2d(y)\n",
    "        if y.shape[0] == 1:\n",
    "            y = y.T\n",
    "        \n",
    "        # Create model architecture\n",
    "        input_size = X.shape[1]\n",
    "        output_size = y.shape[1]\n",
    "        \n",
    "        sizes = (input_size,) + self.hidden_layer_sizes + (output_size,)\n",
    "        self.model = MLP(*sizes, f=self.activation)\n",
    "        \n",
    "        # Create optimizer\n",
    "        optim = self.optimizer(self.model.parameters(), lr=self.lr)\n",
    "        \n",
    "        # Train the model\n",
    "        train(X, y, self.model, self.loss_fn, optim, \n",
    "              epochs=self.epochs, step=self.step, verbose=self.verbose)\n",
    "        \n",
    "        self.is_fitted_ = True\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Generate predictions with the fitted model.\"\"\"\n",
    "        if not self.is_fitted_:\n",
    "            raise ValueError(\"Model not fitted. Call 'fit' first.\")\n",
    "        \n",
    "        # Set model to evaluation mode\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Make predictions\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "            predictions = self.model(X_tensor).numpy()\n",
    "        \n",
    "        # Flatten if single output\n",
    "        if predictions.shape[1] == 1:\n",
    "            predictions = predictions.ravel()\n",
    "            \n",
    "        return predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dbp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
